{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2: Deep Bag-of-Words\n",
    "\n",
    "![words.jpeg](https://cdn-images-1.medium.com/max/1600/0*JpqZhCNsQ_OGaRkB.jpg)\n",
    "\n",
    "<br>\n",
    "\n",
    "In this homework, you will be implementing a deep averaging network, detailed in [Deep Unordered Composition  Rivals Syntactic Methods for Text Classification by Iyyer et al. (2015)](https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf) and training it to do sentiment analysis on the Stanford Sentiment Treebank.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Please use all of the starter code that is provided, do not make any changes to the data processing, evaluation, and training functions. Only add code were you're asked to.**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Read Paper!\n",
    "\n",
    "Read [Deep Unordered Composition  Rivals Syntactic Methods for Text Classification by Iyyer et al. (2015)](https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "Make sure you've downloaded the Stanford Sentiment Treebank that was used in lab. You can find it [here](http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "sst_home = 'data/trees'\n",
    "\n",
    "# Let's do 2-way positive/negative classification instead of 5-way\n",
    "easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "    # so labels of 0 and 1 in te 5-wayclassificaiton are 0 in the 2-way. 3 and 4 are 1, and 2 is none\n",
    "    # because we don't have a neautral class. \n",
    "\n",
    "PADDING = \"<PAD>\"\n",
    "UNKNOWN = \"<UNK>\"\n",
    "max_seq_length = 20\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            example['label'] = easy_label_map[int(line[1])]\n",
    "            if example['label'] is None:\n",
    "                continue\n",
    "            \n",
    "            # Strip out the parse information and the phrase labels---we don't need those here\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')\n",
    "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
    "test_set = load_sst_data(sst_home + '/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Next, we'll extract the vocabulary from the data, index each token, and finally convert the sentences into lists of indexed tokens. We are also padding and truncating all sentences to be of length=20. (Why? Think about how to handle batching. This is certainly not the only way! This is just simple.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def tokenize(string):\n",
    "    return string.split()\n",
    "\n",
    "def build_dictionary(training_datasets):\n",
    "    \"\"\"\n",
    "    Extract vocabulary and build dictionary.\n",
    "    \"\"\"  \n",
    "    word_counter = collections.Counter()\n",
    "    for i, dataset in enumerate(training_datasets):\n",
    "        for example in dataset:\n",
    "            word_counter.update(tokenize(example['text']))\n",
    "        \n",
    "    vocabulary = set([word for word in word_counter])\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
    "        \n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "    return word_indices, len(vocabulary)\n",
    "\n",
    "def sentences_to_padded_index_sequences(word_indices, datasets):\n",
    "    \"\"\"\n",
    "    Annotate datasets with feature vectors. Adding right-sided padding. \n",
    "    \"\"\"\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['text_index_sequence'] = torch.zeros(max_seq_length)\n",
    "\n",
    "            token_sequence = tokenize(example['text'])\n",
    "            padding = max_seq_length - len(token_sequence)\n",
    "\n",
    "            for i in range(max_seq_length):\n",
    "                if i >= len(token_sequence):\n",
    "                    index = word_indices[PADDING]\n",
    "                    pass\n",
    "                else:\n",
    "                    if token_sequence[i] in word_indices:\n",
    "                        index = word_indices[token_sequence[i]]\n",
    "                    else:\n",
    "                        index = word_indices[UNKNOWN]\n",
    "                example['text_index_sequence'][i] = index\n",
    "\n",
    "            example['text_index_sequence'] = example['text_index_sequence'].long().view(1,-1)\n",
    "            example['label'] = torch.LongTensor([example['label']])\n",
    "\n",
    "\n",
    "word_to_ix, vocab_size = build_dictionary([training_set])\n",
    "sentences_to_padded_index_sequences(word_to_ix, [training_set, dev_set, test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training dataset: 6920\n",
      "\n",
      "First padded and indexified example in training data:\n",
      " {'label': tensor([0]), 'text': 'Yet another entry in the sentimental oh-those-wacky-Brits genre that was ushered in by The Full Monty and is still straining to produce another smash hit .', 'text_index_sequence': tensor([[ 7254, 13864,  7736,  6012, 15076, 11566,  7423, 14032,  3388,  9459,\n",
      "          5259,  6012,  7643,  2689,  3834, 15978,  4095,  3994, 13291,  3893]])}\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of training dataset:\", len(training_set))\n",
    "print(\"\\nFirst padded and indexified example in training data:\\n\", training_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Batichify data\n",
    "We're going to be doing mini-batch training. The following code makes data iterators and a batchifying function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the iterator we'll use during training. \n",
    "# It's a generator that gives you one batch at a time.\n",
    "def data_iter(source, batch_size):\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)   \n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [source[index] for index in batch_indices]\n",
    "        yield [source[index] for index in batch_indices]\n",
    "\n",
    "# This is the iterator we use when we're evaluating our model. \n",
    "# It gives a list of batches that you can then iterate through.\n",
    "def eval_iter(source, batch_size):\n",
    "    batches = []\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while start < dataset_size - batch_size:\n",
    "        start += batch_size\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [source[index] for index in batch_indices]\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    return batches\n",
    "\n",
    "# The following function gives batches of vectors and labels, \n",
    "# these are the inputs to your model and loss function\n",
    "def get_batch(batch):\n",
    "    vectors = []\n",
    "    labels = []\n",
    "    for dict in batch:\n",
    "        vectors.append(dict[\"text_index_sequence\"])\n",
    "        labels.append(dict[\"label\"])\n",
    "    return vectors, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We'll be looking at accuracy as our evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function outputs the accuracy on the dataset, we will use it during training.\n",
    "def evaluate(model, data_iter):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(len(data_iter)):\n",
    "        vectors, labels = get_batch(data_iter[i])\n",
    "        vectors = torch.stack(vectors).squeeze()\n",
    "        labels = torch.stack(labels).squeeze()\n",
    "\n",
    "        output = model(vectors)\n",
    "        \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "      \n",
    "    return correct / float(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Training Loop\n",
    "\n",
    "The following function trains the model and reports model accuracy on the train and dev set every 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(batch_size, num_epochs, model, loss_, optim, training_iter, dev_iter,\n",
    "                  train_eval_iter, verbose=True):\n",
    "    step = 0\n",
    "    epoch = 0\n",
    "    total_batches = int(len(training_set) / batch_size)\n",
    "    accuracies = []\n",
    "    while epoch <= num_epochs:\n",
    "        model.train()\n",
    "        vectors, labels = get_batch(next(training_iter)) \n",
    "        vectors = torch.stack(vectors).squeeze() # batch_size, seq_len\n",
    "        labels = torch.stack(labels).squeeze()\n",
    "        \n",
    "    \n",
    "        model.zero_grad()\n",
    "        \n",
    "        output = model(vectors)\n",
    "\n",
    "        lossy = loss_(output, labels)\n",
    "        lossy.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optim.step()\n",
    "        \n",
    "\n",
    "        if step % total_batches == 0:\n",
    "            model.eval()\n",
    "            if epoch % 5 == 0:\n",
    "                train_acc = evaluate(model, train_eval_iter)\n",
    "                eval_acc = evaluate(model, dev_iter)\n",
    "                accuracies.append(eval_acc)\n",
    "                if verbose:\n",
    "                    print(\"Epoch %i; Step %i; Loss %f; Train acc: %f; Dev acc %f\" \n",
    "                          %(epoch, step, lossy.item(),\\\n",
    "                            train_acc, eval_acc))\n",
    "            epoch += 1\n",
    "        step += 1\n",
    "    \n",
    "    best_dev = max(accuracies)\n",
    "    print(\"Best dev accuracy is {}\".format(best_dev))\n",
    "    return best_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Part 1: Implement DAN (40 points)\n",
    "\n",
    "Following the [paper](https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf), implement the Deep Averaging Network (DAN).\n",
    "\n",
    "Implementation details,\n",
    "- Instead of using \\code{tanh} activations however, use \\code{ReLU}. \n",
    "- Make the number of layers a variable, not a fixed value.\n",
    "- Make sure to implement word-dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, \n",
    "                 batch_size, n_layers, drop_rate):\n",
    "        super(DAN, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding_size = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_rate = drop_rate \n",
    "        \n",
    "        self.layer_0 = nn.Linear(embedding_dim, self.hidden_size)\n",
    "        self.decoder = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "        hid_layers = []\n",
    "        for i in range(self.n_layers):\n",
    "            setattr(self, 'l'+str(i), nn.Linear(self.hidden_size, self.hidden_size))\n",
    "            hid_layers.append(nn.Linear(self.hidden_size, self.hidden_size))\n",
    "            hid_layers.append(nn.ReLU())\n",
    "        \n",
    "        self.classifier = nn.Sequential(*hid_layers)\n",
    "        self.classifier.add_module('decoder', self.decoder)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    \n",
    "    def forward(self, input):\n",
    "        text = input\n",
    "        length = input.shape[1]\n",
    "        emb = self.embed(text)\n",
    "        if self.training and self.drop_rate != 0:\n",
    "            rw = Bernoulli(self.drop_rate).sample((emb.shape[1], ))\n",
    "            encoded = emb[:, rw==1].mean(dim=1)\n",
    "        else:\n",
    "            encoded = emb.mean(dim=1)\n",
    "        layer_0 = self.layer_0(encoded)\n",
    "        logits = self.classifier(layer_0)\n",
    "        return self.softmax(logits)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        h0 = torch.zeros(self.batch_size, self.hidden_size)\n",
    "        return h0\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        lin_layers = [self.layer_0, self.decoder]\n",
    "        for i in range(1, self.n_layers):\n",
    "            lin_layers.append(getattr(self, 'l{}'.format(i)))\n",
    "        em_layer =  [self.embed]\n",
    "     \n",
    "        for layer in lin_layers+em_layer:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            if layer in lin_layers:\n",
    "                layer.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model!\n",
    "\n",
    "** Please use the hyperparameters and optimizer provided below. Do not make changes here. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0; Step 0; Loss 0.693146; Train acc: 0.503906; Dev acc 0.531250\n",
      "Epoch 5; Step 135; Loss 0.693138; Train acc: 0.546875; Dev acc 0.585938\n",
      "Epoch 10; Step 270; Loss 0.689602; Train acc: 0.750000; Dev acc 0.699219\n",
      "Epoch 15; Step 405; Loss 0.689462; Train acc: 0.753906; Dev acc 0.703125\n",
      "Epoch 20; Step 540; Loss 0.689611; Train acc: 0.769531; Dev acc 0.714844\n",
      "Epoch 25; Step 675; Loss 0.689673; Train acc: 0.796875; Dev acc 0.734375\n",
      "Epoch 30; Step 810; Loss 0.689362; Train acc: 0.808594; Dev acc 0.718750\n",
      "Best dev accuracy is 0.734375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.734375"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters \n",
    "input_size = vocab_size\n",
    "num_labels = 2\n",
    "hidden_dim = 24\n",
    "embedding_dim = 8\n",
    "batch_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "drop_rate = 0.4\n",
    "num_epochs = 30\n",
    "\n",
    "\n",
    "# Build and initialize the model\n",
    "dan = DAN(vocab_size, embedding_dim, hidden_dim, num_labels, batch_size, num_layers, drop_rate)\n",
    "dan.init_weights()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(dan.parameters(), lr=learning_rate)\n",
    "\n",
    "# Build data iterators\n",
    "training_iter = data_iter(training_set, batch_size)\n",
    "train_eval_iter = eval_iter(training_set[0:500], batch_size)\n",
    "dev_iter = eval_iter(dev_set[:500], batch_size)\n",
    "\n",
    "# Train the model\n",
    "training_loop(batch_size, num_epochs, dan, loss, optimizer, training_iter, dev_iter, train_eval_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Part 2: Hyperparameter tuning (40 points)\n",
    "\n",
    "Tune the DAN for learning rate, number of layers, and drop-out rate. Select a range for each parameter and then do a random search over these hyperparameters, trying a minimum 5 permutations of hyperparameters. Report results and the best hyperparameters you found. Do you see any patterns in your results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_(batch_size, num_epochs, model, loss_, optim, training_iter, dev_iter,\n",
    "                  train_eval_iter, verbose=True):\n",
    "    step = 0\n",
    "    epoch = 0\n",
    "    total_batches = int(len(training_set) / batch_size)\n",
    "    accuracies = []\n",
    "    while epoch <= num_epochs:\n",
    "        model.train()\n",
    "        vectors, labels = get_batch(next(training_iter)) \n",
    "        vectors = torch.stack(vectors).squeeze() # batch_size, seq_len\n",
    "        labels = torch.stack(labels).squeeze()\n",
    "        \n",
    "    \n",
    "        model.zero_grad()\n",
    "        \n",
    "        output = model(vectors)\n",
    "\n",
    "        lossy = loss_(output, labels)\n",
    "        lossy.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optim.step()\n",
    "        \n",
    "\n",
    "        if step % total_batches == 0:\n",
    "            model.eval()\n",
    "            if epoch % 5 == 0:\n",
    "                train_acc = evaluate(model, train_eval_iter)\n",
    "                eval_acc = evaluate(model, dev_iter)\n",
    "                accuracies.append(eval_acc)\n",
    "            epoch += 1\n",
    "        step += 1\n",
    "    \n",
    "    best_dev = max(accuracies)\n",
    "    print(\"Best dev accuracy is {}\".format(best_dev))\n",
    "    return best_dev, lossy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate : 0.001 num_layers : 1 drop_rate : 0.1\n",
      "Best dev accuracy is 0.453125\n",
      "learning_rate : 0.001 num_layers : 1 drop_rate : 0.2\n",
      "Best dev accuracy is 0.48828125\n",
      "learning_rate : 0.001 num_layers : 1 drop_rate : 0.3\n",
      "Best dev accuracy is 0.7578125\n",
      "learning_rate : 0.001 num_layers : 1 drop_rate : 0.4\n",
      "Best dev accuracy is 0.79296875\n",
      "learning_rate : 0.001 num_layers : 1 drop_rate : 0.5\n",
      "Best dev accuracy is 0.8203125\n",
      "learning_rate : 0.001 num_layers : 1 drop_rate : 0.6\n",
      "Best dev accuracy is 0.796875\n",
      "learning_rate : 0.001 num_layers : 2 drop_rate : 0.1\n",
      "Best dev accuracy is 0.4609375\n",
      "learning_rate : 0.001 num_layers : 2 drop_rate : 0.2\n",
      "Best dev accuracy is 0.546875\n",
      "learning_rate : 0.001 num_layers : 2 drop_rate : 0.3\n",
      "Best dev accuracy is 0.7734375\n",
      "learning_rate : 0.001 num_layers : 2 drop_rate : 0.4\n",
      "Best dev accuracy is 0.80859375\n",
      "learning_rate : 0.001 num_layers : 2 drop_rate : 0.5\n",
      "Best dev accuracy is 0.79296875\n",
      "learning_rate : 0.001 num_layers : 2 drop_rate : 0.6\n",
      "Best dev accuracy is 0.76171875\n",
      "learning_rate : 0.001 num_layers : 3 drop_rate : 0.1\n",
      "Best dev accuracy is 0.50390625\n",
      "learning_rate : 0.001 num_layers : 3 drop_rate : 0.2\n",
      "Best dev accuracy is 0.52734375\n",
      "learning_rate : 0.001 num_layers : 3 drop_rate : 0.3\n",
      "Best dev accuracy is 0.765625\n",
      "learning_rate : 0.001 num_layers : 3 drop_rate : 0.4\n",
      "Best dev accuracy is 0.76171875\n",
      "learning_rate : 0.001 num_layers : 3 drop_rate : 0.5\n",
      "Best dev accuracy is 0.75\n",
      "learning_rate : 0.001 num_layers : 3 drop_rate : 0.6\n",
      "Best dev accuracy is 0.7421875\n",
      "learning_rate : 0.001 num_layers : 4 drop_rate : 0.1\n",
      "Best dev accuracy is 0.53125\n",
      "learning_rate : 0.001 num_layers : 4 drop_rate : 0.2\n",
      "Best dev accuracy is 0.53515625\n",
      "learning_rate : 0.001 num_layers : 4 drop_rate : 0.3\n",
      "Best dev accuracy is 0.53515625\n",
      "learning_rate : 0.001 num_layers : 4 drop_rate : 0.4\n",
      "Best dev accuracy is 0.78125\n",
      "learning_rate : 0.001 num_layers : 4 drop_rate : 0.5\n",
      "Best dev accuracy is 0.74609375\n",
      "learning_rate : 0.001 num_layers : 4 drop_rate : 0.6\n",
      "Best dev accuracy is 0.76953125\n",
      "learning_rate : 0.001 num_layers : 5 drop_rate : 0.1\n",
      "Best dev accuracy is 0.55078125\n",
      "learning_rate : 0.001 num_layers : 5 drop_rate : 0.2\n",
      "Best dev accuracy is 0.53125\n",
      "learning_rate : 0.001 num_layers : 5 drop_rate : 0.3\n",
      "Best dev accuracy is 0.5234375\n",
      "learning_rate : 0.001 num_layers : 5 drop_rate : 0.4\n",
      "Best dev accuracy is 0.7578125\n",
      "learning_rate : 0.001 num_layers : 5 drop_rate : 0.5\n",
      "Best dev accuracy is 0.69140625\n",
      "learning_rate : 0.001 num_layers : 5 drop_rate : 0.6\n",
      "Best dev accuracy is 0.73828125\n",
      "learning_rate : 0.001 num_layers : 6 drop_rate : 0.1\n",
      "Best dev accuracy is 0.54296875\n",
      "learning_rate : 0.001 num_layers : 6 drop_rate : 0.2\n",
      "Best dev accuracy is 0.49609375\n",
      "learning_rate : 0.001 num_layers : 6 drop_rate : 0.3\n",
      "Best dev accuracy is 0.55078125\n",
      "learning_rate : 0.001 num_layers : 6 drop_rate : 0.4\n",
      "Best dev accuracy is 0.7421875\n",
      "learning_rate : 0.001 num_layers : 6 drop_rate : 0.5\n",
      "Best dev accuracy is 0.546875\n",
      "learning_rate : 0.001 num_layers : 6 drop_rate : 0.6\n",
      "Best dev accuracy is 0.73828125\n",
      "learning_rate : 0.005 num_layers : 1 drop_rate : 0.1\n",
      "Best dev accuracy is 0.46875\n",
      "learning_rate : 0.005 num_layers : 1 drop_rate : 0.2\n",
      "Best dev accuracy is 0.51171875\n",
      "learning_rate : 0.005 num_layers : 1 drop_rate : 0.3\n",
      "Best dev accuracy is 0.703125\n",
      "learning_rate : 0.005 num_layers : 1 drop_rate : 0.4\n",
      "Best dev accuracy is 0.7109375\n",
      "learning_rate : 0.005 num_layers : 1 drop_rate : 0.5\n",
      "Best dev accuracy is 0.79296875\n",
      "learning_rate : 0.005 num_layers : 1 drop_rate : 0.6\n",
      "Best dev accuracy is 0.78515625\n",
      "learning_rate : 0.005 num_layers : 2 drop_rate : 0.1\n",
      "Best dev accuracy is 0.52734375\n",
      "learning_rate : 0.005 num_layers : 2 drop_rate : 0.2\n",
      "Best dev accuracy is 0.49609375\n",
      "learning_rate : 0.005 num_layers : 2 drop_rate : 0.3\n",
      "Best dev accuracy is 0.6328125\n",
      "learning_rate : 0.005 num_layers : 2 drop_rate : 0.4\n",
      "Best dev accuracy is 0.78125\n",
      "learning_rate : 0.005 num_layers : 2 drop_rate : 0.5\n",
      "Best dev accuracy is 0.703125\n",
      "learning_rate : 0.005 num_layers : 2 drop_rate : 0.6\n",
      "Best dev accuracy is 0.7734375\n",
      "learning_rate : 0.005 num_layers : 3 drop_rate : 0.1\n",
      "Best dev accuracy is 0.5078125\n",
      "learning_rate : 0.005 num_layers : 3 drop_rate : 0.2\n",
      "Best dev accuracy is 0.53515625\n",
      "learning_rate : 0.005 num_layers : 3 drop_rate : 0.3\n",
      "Best dev accuracy is 0.6875\n",
      "learning_rate : 0.005 num_layers : 3 drop_rate : 0.4\n",
      "Best dev accuracy is 0.734375\n",
      "learning_rate : 0.005 num_layers : 3 drop_rate : 0.5\n",
      "Best dev accuracy is 0.7265625\n",
      "learning_rate : 0.005 num_layers : 3 drop_rate : 0.6\n",
      "Best dev accuracy is 0.75\n",
      "learning_rate : 0.005 num_layers : 4 drop_rate : 0.1\n",
      "Best dev accuracy is 0.48828125\n",
      "learning_rate : 0.005 num_layers : 4 drop_rate : 0.2\n",
      "Best dev accuracy is 0.5390625\n",
      "learning_rate : 0.005 num_layers : 4 drop_rate : 0.3\n",
      "Best dev accuracy is 0.734375\n",
      "learning_rate : 0.005 num_layers : 4 drop_rate : 0.4\n",
      "Best dev accuracy is 0.74609375\n",
      "learning_rate : 0.005 num_layers : 4 drop_rate : 0.5\n",
      "Best dev accuracy is 0.74609375\n",
      "learning_rate : 0.005 num_layers : 4 drop_rate : 0.6\n",
      "Best dev accuracy is 0.75\n",
      "learning_rate : 0.005 num_layers : 5 drop_rate : 0.1\n",
      "Best dev accuracy is 0.54296875\n",
      "learning_rate : 0.005 num_layers : 5 drop_rate : 0.2\n",
      "Best dev accuracy is 0.5546875\n",
      "learning_rate : 0.005 num_layers : 5 drop_rate : 0.3\n",
      "Best dev accuracy is 0.6328125\n",
      "learning_rate : 0.005 num_layers : 5 drop_rate : 0.4\n",
      "Best dev accuracy is 0.671875\n",
      "learning_rate : 0.005 num_layers : 5 drop_rate : 0.5\n",
      "Best dev accuracy is 0.76171875\n",
      "learning_rate : 0.005 num_layers : 5 drop_rate : 0.6\n",
      "Best dev accuracy is 0.74609375\n",
      "learning_rate : 0.005 num_layers : 6 drop_rate : 0.1\n",
      "Best dev accuracy is 0.4765625\n",
      "learning_rate : 0.005 num_layers : 6 drop_rate : 0.2\n",
      "Best dev accuracy is 0.48828125\n",
      "learning_rate : 0.005 num_layers : 6 drop_rate : 0.3\n",
      "Best dev accuracy is 0.53515625\n",
      "learning_rate : 0.005 num_layers : 6 drop_rate : 0.4\n",
      "Best dev accuracy is 0.734375\n",
      "learning_rate : 0.005 num_layers : 6 drop_rate : 0.5\n",
      "Best dev accuracy is 0.73046875\n",
      "learning_rate : 0.005 num_layers : 6 drop_rate : 0.6\n",
      "Best dev accuracy is 0.6953125\n",
      "learning_rate : 0.01 num_layers : 1 drop_rate : 0.1\n",
      "Best dev accuracy is 0.5234375\n",
      "learning_rate : 0.01 num_layers : 1 drop_rate : 0.2\n",
      "Best dev accuracy is 0.51953125\n",
      "learning_rate : 0.01 num_layers : 1 drop_rate : 0.3\n",
      "Best dev accuracy is 0.70703125\n",
      "learning_rate : 0.01 num_layers : 1 drop_rate : 0.4\n",
      "Best dev accuracy is 0.75390625\n",
      "learning_rate : 0.01 num_layers : 1 drop_rate : 0.5\n",
      "Best dev accuracy is 0.71484375\n",
      "learning_rate : 0.01 num_layers : 1 drop_rate : 0.6\n",
      "Best dev accuracy is 0.78515625\n",
      "learning_rate : 0.01 num_layers : 2 drop_rate : 0.1\n",
      "Best dev accuracy is 0.48046875\n",
      "learning_rate : 0.01 num_layers : 2 drop_rate : 0.2\n",
      "Best dev accuracy is 0.515625\n",
      "learning_rate : 0.01 num_layers : 2 drop_rate : 0.3\n",
      "Best dev accuracy is 0.703125\n",
      "learning_rate : 0.01 num_layers : 2 drop_rate : 0.4\n",
      "Best dev accuracy is 0.73828125\n",
      "learning_rate : 0.01 num_layers : 2 drop_rate : 0.5\n",
      "Best dev accuracy is 0.66796875\n",
      "learning_rate : 0.01 num_layers : 2 drop_rate : 0.6\n",
      "Best dev accuracy is 0.74609375\n",
      "learning_rate : 0.01 num_layers : 3 drop_rate : 0.1\n",
      "Best dev accuracy is 0.5078125\n",
      "learning_rate : 0.01 num_layers : 3 drop_rate : 0.2\n",
      "Best dev accuracy is 0.53125\n",
      "learning_rate : 0.01 num_layers : 3 drop_rate : 0.3\n",
      "Best dev accuracy is 0.78125\n",
      "learning_rate : 0.01 num_layers : 3 drop_rate : 0.4\n",
      "Best dev accuracy is 0.7421875\n",
      "learning_rate : 0.01 num_layers : 3 drop_rate : 0.5\n",
      "Best dev accuracy is 0.6796875\n",
      "learning_rate : 0.01 num_layers : 3 drop_rate : 0.6\n",
      "Best dev accuracy is 0.7890625\n",
      "learning_rate : 0.01 num_layers : 4 drop_rate : 0.1\n",
      "Best dev accuracy is 0.48046875\n",
      "learning_rate : 0.01 num_layers : 4 drop_rate : 0.2\n",
      "Best dev accuracy is 0.5390625\n",
      "learning_rate : 0.01 num_layers : 4 drop_rate : 0.3\n",
      "Best dev accuracy is 0.7421875\n",
      "learning_rate : 0.01 num_layers : 4 drop_rate : 0.4\n",
      "Best dev accuracy is 0.69140625\n",
      "learning_rate : 0.01 num_layers : 4 drop_rate : 0.5\n",
      "Best dev accuracy is 0.76953125\n",
      "learning_rate : 0.01 num_layers : 4 drop_rate : 0.6\n",
      "Best dev accuracy is 0.78125\n",
      "learning_rate : 0.01 num_layers : 5 drop_rate : 0.1\n",
      "Best dev accuracy is 0.53515625\n",
      "learning_rate : 0.01 num_layers : 5 drop_rate : 0.2\n",
      "Best dev accuracy is 0.54296875\n",
      "learning_rate : 0.01 num_layers : 5 drop_rate : 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best dev accuracy is 0.64453125\n",
      "learning_rate : 0.01 num_layers : 5 drop_rate : 0.4\n",
      "Best dev accuracy is 0.67578125\n",
      "learning_rate : 0.01 num_layers : 5 drop_rate : 0.5\n",
      "Best dev accuracy is 0.74609375\n",
      "learning_rate : 0.01 num_layers : 5 drop_rate : 0.6\n",
      "Best dev accuracy is 0.71484375\n",
      "learning_rate : 0.01 num_layers : 6 drop_rate : 0.1\n",
      "Best dev accuracy is 0.53515625\n",
      "learning_rate : 0.01 num_layers : 6 drop_rate : 0.2\n",
      "Best dev accuracy is 0.5390625\n",
      "learning_rate : 0.01 num_layers : 6 drop_rate : 0.3\n",
      "Best dev accuracy is 0.625\n",
      "learning_rate : 0.01 num_layers : 6 drop_rate : 0.4\n",
      "Best dev accuracy is 0.6171875\n",
      "learning_rate : 0.01 num_layers : 6 drop_rate : 0.5\n",
      "Best dev accuracy is 0.73828125\n",
      "learning_rate : 0.01 num_layers : 6 drop_rate : 0.6\n",
      "Best dev accuracy is 0.70703125\n",
      "learning_rate : 0.05 num_layers : 1 drop_rate : 0.1\n",
      "Best dev accuracy is 0.5234375\n",
      "learning_rate : 0.05 num_layers : 1 drop_rate : 0.2\n",
      "Best dev accuracy is 0.6015625\n",
      "learning_rate : 0.05 num_layers : 1 drop_rate : 0.3\n",
      "Best dev accuracy is 0.6171875\n",
      "learning_rate : 0.05 num_layers : 1 drop_rate : 0.4\n",
      "Best dev accuracy is 0.609375\n",
      "learning_rate : 0.05 num_layers : 1 drop_rate : 0.5\n",
      "Best dev accuracy is 0.59765625\n",
      "learning_rate : 0.05 num_layers : 1 drop_rate : 0.6\n",
      "Best dev accuracy is 0.6484375\n",
      "learning_rate : 0.05 num_layers : 2 drop_rate : 0.1\n",
      "Best dev accuracy is 0.46875\n",
      "learning_rate : 0.05 num_layers : 2 drop_rate : 0.2\n",
      "Best dev accuracy is 0.49609375\n",
      "learning_rate : 0.05 num_layers : 2 drop_rate : 0.3\n",
      "Best dev accuracy is 0.5078125\n",
      "learning_rate : 0.05 num_layers : 2 drop_rate : 0.4\n",
      "Best dev accuracy is 0.55078125\n",
      "learning_rate : 0.05 num_layers : 2 drop_rate : 0.5\n",
      "Best dev accuracy is 0.57421875\n",
      "learning_rate : 0.05 num_layers : 2 drop_rate : 0.6\n",
      "Best dev accuracy is 0.57421875\n",
      "learning_rate : 0.05 num_layers : 3 drop_rate : 0.1\n",
      "Best dev accuracy is 0.48046875\n",
      "learning_rate : 0.05 num_layers : 3 drop_rate : 0.2\n",
      "Best dev accuracy is 0.47265625\n",
      "learning_rate : 0.05 num_layers : 3 drop_rate : 0.3\n",
      "Best dev accuracy is 0.56640625\n",
      "learning_rate : 0.05 num_layers : 3 drop_rate : 0.4\n",
      "Best dev accuracy is 0.51953125\n",
      "learning_rate : 0.05 num_layers : 3 drop_rate : 0.5\n",
      "Best dev accuracy is 0.59375\n",
      "learning_rate : 0.05 num_layers : 3 drop_rate : 0.6\n",
      "Best dev accuracy is 0.5546875\n",
      "learning_rate : 0.05 num_layers : 4 drop_rate : 0.1\n",
      "Best dev accuracy is 0.45703125\n",
      "learning_rate : 0.05 num_layers : 4 drop_rate : 0.2\n",
      "Best dev accuracy is 0.46484375\n",
      "learning_rate : 0.05 num_layers : 4 drop_rate : 0.3\n",
      "Best dev accuracy is 0.55859375\n",
      "learning_rate : 0.05 num_layers : 4 drop_rate : 0.4\n",
      "Best dev accuracy is 0.5234375\n",
      "learning_rate : 0.05 num_layers : 4 drop_rate : 0.5\n",
      "Best dev accuracy is 0.546875\n",
      "learning_rate : 0.05 num_layers : 4 drop_rate : 0.6\n",
      "Best dev accuracy is 0.4921875\n",
      "learning_rate : 0.05 num_layers : 5 drop_rate : 0.1\n",
      "Best dev accuracy is 0.48046875\n",
      "learning_rate : 0.05 num_layers : 5 drop_rate : 0.2\n",
      "Best dev accuracy is 0.45703125\n",
      "learning_rate : 0.05 num_layers : 5 drop_rate : 0.3\n",
      "Best dev accuracy is 0.5703125\n",
      "learning_rate : 0.05 num_layers : 5 drop_rate : 0.4\n",
      "Best dev accuracy is 0.53125\n",
      "learning_rate : 0.05 num_layers : 5 drop_rate : 0.5\n",
      "Best dev accuracy is 0.47265625\n",
      "learning_rate : 0.05 num_layers : 5 drop_rate : 0.6\n",
      "Best dev accuracy is 0.55859375\n",
      "learning_rate : 0.05 num_layers : 6 drop_rate : 0.1\n",
      "Best dev accuracy is 0.46484375\n",
      "learning_rate : 0.05 num_layers : 6 drop_rate : 0.2\n",
      "Best dev accuracy is 0.46875\n",
      "learning_rate : 0.05 num_layers : 6 drop_rate : 0.3\n",
      "Best dev accuracy is 0.51171875\n",
      "learning_rate : 0.05 num_layers : 6 drop_rate : 0.4\n",
      "Best dev accuracy is 0.53515625\n",
      "learning_rate : 0.05 num_layers : 6 drop_rate : 0.5\n",
      "Best dev accuracy is 0.5546875\n",
      "learning_rate : 0.05 num_layers : 6 drop_rate : 0.6\n",
      "Best dev accuracy is 0.55078125\n",
      "learning_rate : 0.1 num_layers : 1 drop_rate : 0.1\n",
      "Best dev accuracy is 0.484375\n",
      "learning_rate : 0.1 num_layers : 1 drop_rate : 0.2\n",
      "Best dev accuracy is 0.53125\n",
      "learning_rate : 0.1 num_layers : 1 drop_rate : 0.3\n",
      "Best dev accuracy is 0.5078125\n",
      "learning_rate : 0.1 num_layers : 1 drop_rate : 0.4\n",
      "Best dev accuracy is 0.54296875\n",
      "learning_rate : 0.1 num_layers : 1 drop_rate : 0.5\n",
      "Best dev accuracy is 0.53125\n",
      "learning_rate : 0.1 num_layers : 1 drop_rate : 0.6\n",
      "Best dev accuracy is 0.5859375\n",
      "learning_rate : 0.1 num_layers : 2 drop_rate : 0.1\n",
      "Best dev accuracy is 0.47265625\n",
      "learning_rate : 0.1 num_layers : 2 drop_rate : 0.2\n",
      "Best dev accuracy is 0.5546875\n",
      "learning_rate : 0.1 num_layers : 2 drop_rate : 0.3\n",
      "Best dev accuracy is 0.546875\n",
      "learning_rate : 0.1 num_layers : 2 drop_rate : 0.4\n",
      "Best dev accuracy is 0.55078125\n",
      "learning_rate : 0.1 num_layers : 2 drop_rate : 0.5\n",
      "Best dev accuracy is 0.54296875\n",
      "learning_rate : 0.1 num_layers : 2 drop_rate : 0.6\n",
      "Best dev accuracy is 0.53515625\n",
      "learning_rate : 0.1 num_layers : 3 drop_rate : 0.1\n",
      "Best dev accuracy is 0.5\n",
      "learning_rate : 0.1 num_layers : 3 drop_rate : 0.2\n",
      "Best dev accuracy is 0.515625\n",
      "learning_rate : 0.1 num_layers : 3 drop_rate : 0.3\n",
      "Best dev accuracy is 0.53515625\n",
      "learning_rate : 0.1 num_layers : 3 drop_rate : 0.4\n",
      "Best dev accuracy is 0.51953125\n",
      "learning_rate : 0.1 num_layers : 3 drop_rate : 0.5\n",
      "Best dev accuracy is 0.52734375\n",
      "learning_rate : 0.1 num_layers : 3 drop_rate : 0.6\n",
      "Best dev accuracy is 0.55078125\n",
      "learning_rate : 0.1 num_layers : 4 drop_rate : 0.1\n",
      "Best dev accuracy is 0.515625\n",
      "learning_rate : 0.1 num_layers : 4 drop_rate : 0.2\n",
      "Best dev accuracy is 0.54296875\n",
      "learning_rate : 0.1 num_layers : 4 drop_rate : 0.3\n",
      "Best dev accuracy is 0.5625\n",
      "learning_rate : 0.1 num_layers : 4 drop_rate : 0.4\n",
      "Best dev accuracy is 0.53125\n",
      "learning_rate : 0.1 num_layers : 4 drop_rate : 0.5\n",
      "Best dev accuracy is 0.515625\n",
      "learning_rate : 0.1 num_layers : 4 drop_rate : 0.6\n",
      "Best dev accuracy is 0.5546875\n",
      "learning_rate : 0.1 num_layers : 5 drop_rate : 0.1\n",
      "Best dev accuracy is 0.484375\n",
      "learning_rate : 0.1 num_layers : 5 drop_rate : 0.2\n",
      "Best dev accuracy is 0.48046875\n",
      "learning_rate : 0.1 num_layers : 5 drop_rate : 0.3\n",
      "Best dev accuracy is 0.5234375\n",
      "learning_rate : 0.1 num_layers : 5 drop_rate : 0.4\n",
      "Best dev accuracy is 0.5546875\n",
      "learning_rate : 0.1 num_layers : 5 drop_rate : 0.5\n",
      "Best dev accuracy is 0.54296875\n",
      "learning_rate : 0.1 num_layers : 5 drop_rate : 0.6\n",
      "Best dev accuracy is 0.54296875\n",
      "learning_rate : 0.1 num_layers : 6 drop_rate : 0.1\n",
      "Best dev accuracy is 0.50390625\n",
      "learning_rate : 0.1 num_layers : 6 drop_rate : 0.2\n",
      "Best dev accuracy is 0.47265625\n",
      "learning_rate : 0.1 num_layers : 6 drop_rate : 0.3\n",
      "Best dev accuracy is 0.51171875\n",
      "learning_rate : 0.1 num_layers : 6 drop_rate : 0.4\n",
      "Best dev accuracy is 0.52734375\n",
      "learning_rate : 0.1 num_layers : 6 drop_rate : 0.5\n",
      "Best dev accuracy is 0.4921875\n",
      "learning_rate : 0.1 num_layers : 6 drop_rate : 0.6\n",
      "Best dev accuracy is 0.53125\n"
     ]
    }
   ],
   "source": [
    "# torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "# Hyperparameters \n",
    "input_size = vocab_size\n",
    "num_labels = 2\n",
    "hidden_dim = 24\n",
    "embedding_dim = 8\n",
    "batch_size = 256\n",
    "# num_layers = 2\n",
    "# learning_rate = 0.001\n",
    "# drop_rate = 0.4\n",
    "num_epochs = 30\n",
    "\n",
    "def grid_search(learning_rate, num_layers, drop_rate, vocab_size = input_size,  embedding_dim = 8, hidden_dim = 24, \n",
    "                num_labels = 2, batch_size = 256):\n",
    "    \n",
    "    # Build and initialize the model\n",
    "    dan = DAN(vocab_size, embedding_dim, hidden_dim, num_labels, batch_size, num_layers, drop_rate)\n",
    "    dan.init_weights()\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss = nn.CrossEntropyLoss()  \n",
    "    optimizer = torch.optim.Adam(dan.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Build data iterators\n",
    "    training_iter = data_iter(training_set, batch_size)\n",
    "    train_eval_iter = eval_iter(training_set[0:500], batch_size)\n",
    "    dev_iter = eval_iter(dev_set[:500], batch_size)\n",
    "\n",
    "    # Train the model\n",
    "    best_acc, test_loss =training_loop_(batch_size, num_epochs, dan, loss, optimizer, training_iter, dev_iter, train_eval_iter)\n",
    "    return best_acc, test_loss\n",
    "    \n",
    "hyperp = {'learning_rate' : [0.001, 0.005, 0.01, 0.05, 0.1],  \n",
    " 'num_layers' : [1, 2, 3, 4, 5, 6], \n",
    " 'drop_rate' : [0.1, 0.2, 0.3, 0.4, 0.5, 0.6], \n",
    "}\n",
    "\n",
    "result = {'learning_rate' : [],  \n",
    " 'num_layers' : [], \n",
    " 'drop_rate' : [], \n",
    " 'best_acc' : [], \n",
    " 'test_loss' : []\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for lr in hyperp['learning_rate']:\n",
    "    for n_layer in hyperp['num_layers']:\n",
    "        for drop in hyperp['drop_rate']:\n",
    "            print('learning_rate : '+ str(lr) , 'num_layers : '+ str(n_layer), 'drop_rate : '+ str(drop))\n",
    "            best_acc, test_loss = grid_search(lr, n_layer, drop)\n",
    "            result['learning_rate'] += [lr]\n",
    "            result['num_layers']+= [n_layer]\n",
    "            result['drop_rate']+= [drop]\n",
    "            result['best_acc']+= [best_acc]\n",
    "            result['test_loss']+= [test_loss]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "# df = pd.DataFrame(result)\n",
    "# df.to_csv('grid_search_result')\n",
    "df = pd.read_csv('grid_search_result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "1. The best parameter is when the learning_rate = 0.001, num_layers = 1, drop_rate = 0.5. It's more likely to get higher test performance when optimization learning_rate is smaller with a fewer number of layers and higher drop rate. Smaller learning rate means that it takes much smaller steps to reach the local or global minimum. Higher drop rate and fewer layers mean that the model will generalize well with less variance and avoid too much overfitting. And it seems that this model has some overfitting problem, and this is why it still performs well when the drop rate is set to 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>drop_rate</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>test_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.820312</td>\n",
       "      <td>0.689723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.808594</td>\n",
       "      <td>0.689347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.689615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.792969</td>\n",
       "      <td>0.689437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.792969</td>\n",
       "      <td>0.689325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.005</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.792969</td>\n",
       "      <td>0.689289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.010</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.789062</td>\n",
       "      <td>0.689506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.785156</td>\n",
       "      <td>0.689283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.005</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.785156</td>\n",
       "      <td>0.689281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.689472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning_rate  num_layers  drop_rate  best_acc  test_loss\n",
       "4           0.001           1        0.5  0.820312   0.689723\n",
       "9           0.001           2        0.4  0.808594   0.689347\n",
       "5           0.001           1        0.6  0.796875   0.689615\n",
       "3           0.001           1        0.4  0.792969   0.689437\n",
       "10          0.001           2        0.5  0.792969   0.689325\n",
       "40          0.005           1        0.5  0.792969   0.689289\n",
       "89          0.010           3        0.6  0.789062   0.689506\n",
       "77          0.010           1        0.6  0.785156   0.689283\n",
       "41          0.005           1        0.6  0.785156   0.689281\n",
       "21          0.001           4        0.4  0.781250   0.689472"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.iloc[:, 1:]\n",
    "df.sort_values('best_acc', axis=0, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. You can tell from the table below that a fixed epoch method might not converge a certain paramter settings, e.g. first example in the table where test_loss = NaN. It might be good to set up another converge criterion.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>drop_rate</th>\n",
       "      <th>best_acc</th>\n",
       "      <th>test_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.488281</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.757812</td>\n",
       "      <td>0.689484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.792969</td>\n",
       "      <td>0.689437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.820312</td>\n",
       "      <td>0.689723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.689615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.460938</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.773438</td>\n",
       "      <td>0.689738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.808594</td>\n",
       "      <td>0.689347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learning_rate  num_layers  drop_rate  best_acc  test_loss\n",
       "0          0.001           1        0.1  0.453125        NaN\n",
       "1          0.001           1        0.2  0.488281        NaN\n",
       "2          0.001           1        0.3  0.757812   0.689484\n",
       "3          0.001           1        0.4  0.792969   0.689437\n",
       "4          0.001           1        0.5  0.820312   0.689723\n",
       "5          0.001           1        0.6  0.796875   0.689615\n",
       "6          0.001           2        0.1  0.460938        NaN\n",
       "7          0.001           2        0.2  0.546875        NaN\n",
       "8          0.001           2        0.3  0.773438   0.689738\n",
       "9          0.001           2        0.4  0.808594   0.689347"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Part 3: short questions (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Name, and briefly describe, 3 other possible composition functions, instead of the DAN, you could use to build sentence representations.\n",
    "2. Explain how dropout regularizes a model.\n",
    "3. What are the shortcomings for training for a fixed number of epochs? Give an alternative.\n",
    "4. Explain why you might use random search rather than grid search.\n",
    "\n",
    "_Bonus (5 points): briefly describe the Nelder–Mead method and how you might use it to do hyperparamter tuning. What are the tradeoffs between using Nelder-Mead vs random search?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Name, and briefly describe, 3 other possible composition functions, instead of the DAN, you could use to build sentence representations.\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "Recursive Neural Networks (RecNN) : According to wikipedia, \"A recursive neural network is created by applying the same set of weights recursively over a structured input, to produce a structured prediction over variable-size input structures, or a scalar prediction on it, by traversing a given structure in topological order\". And it works well with the word combination. \"In the case of a RNN we are feeding the hidden layers from the previous step as an additional input into the next step.\"\n",
    "\n",
    "Convolutional Neural Networks (CNN) : \"CNN is looking for the same patterns over different regions of sentences. CNN take a fixed size input and generate fixed-size outputs  [reference](https://datascience.stackexchange.com/questions/11619/rnn-vs-cnn-at-a-high-level)\" \"CNN encoder performs convolution and pooling operations on an input sentence, then uses a fully connected layer to produce a fixed-length encoding of the sentence. This encoding vector is then fed into a long short-term memory (LSTM) recurrent network to produce a target sentence. [reference](http://aclweb.org/anthology/D17-1254)\"\n",
    "\n",
    "Recurrent Neural Networks (RNN) : \"A recurrent neural network (RNN) is a class of artificial neural network where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs [reference](https://en.wikipedia.org/wiki/Recurrent_neural_network)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explain how dropout regularizes a model.\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "Dropout: Dropping some units and their connections from the network at random during training. In this case, it tries to decrease the complexity of the model. In this case, it will avoid over fits the training data and decrease bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What are the shortcomings for training for a fixed number of epochs? Give an alternative.\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "The shortcomings for a fixed number of epochs is that after a fixed epochs we are not sure about whether the optimization and loss converges or not. Thus, an alternative is that, rather than setting a fixed epochs, we can determine a convergence criterion, such as the optimization steps end when the loss function rarely decreases.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Explain why you might use random search rather than grid search.\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "As random values are selected at each instance, it is highly likely that the whole of action space has been reached because of the randomness, which takes a huge amount of time to cover every aspect of the combination during grid search since it uniformly searches the whole space. This works best under the assumption that not all hyperparameters are equally important. In this search pattern, random combinations of parameters are considered in every iteration. The chances of finding the optimal parameter are comparatively higher in random search because of the random search pattern where the model might end up being trained on the optimised parameters without any aliasing. [reference](https://www.analyticsindiamag.com/why-is-random-search-better-than-grid-search-for-machine-learning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
